# LUME-services Model Template
This repository provides a [cookiecutter](https://cookiecutter.readthedocs.io/en/stable/) model template for compatibility with [LUME-services](https://slaclab.github.io/lume-services/) orchestration tooling.


## Tools
* conda
* click for cli dev
* versioneer (cookiecutter runs install during post-installation hook)
* [pre-commit](https://pre-commit.com/) hook with black and flake8
* pytest with coverage
* Common github actions
* [pydantic](https://pydantic-docs.helpmanual.io/) for use with data structures



# Demo

This demo walks through the creation of a model compatible with [LUME-services](https://slaclab.github.io/lume-services/) tooling. You will:
1. Create a GitHub repository for the demo model.
2. Build a templated project using [`lume-services-model-template`]()
3. 


## 1. Create a repository for your project on GitHub
Using your GitHub account, create an repository named `my-model` (feel free to sub this with whatever you'd like).

## 2: Create project 

Clone `lume-model-services-template`.

```
cookiecutter template -o DIRECTORY_WHERE_YOU_WANT_TO_CREATE_REPO
```

Answer the prompts
```
author: YOUR NAME
email: YOUR EMAIL ADDRESS
github_username: YOUR GIHUB USERNAME
github_url: URL OF GITHUB REPO YOU'VE JUST CREATED
project_name: My Model
```
Use autogenerated in brackets by just pressing enter.
```
repo_name [my-model]:
project_slug [my_model]:
model_class [MyModel]:
```
Use example variables file packaged with this repository. This requires the full path to the file as the hook runs in the root of the generated project.
```
model_config_file: /full/path/to/lume-services-model-template/examples/variables.yml
```

Now, navigate to the directory where you've created your repository:
```
cd DIRECTORY_WHERE_YOU_WANT_TO_CREATE_REPO/my-model
```

## 3. Create a repository on GitHub and configure generated repo to use as origin

```
git remote add origin git@github.com:{YOUR_GITHUB_USERNAME}/my-project.git
git push --set-upstream origin main
```

## 4. Set up model

Replace ellipses in `my_package/model.py` with:
```python
self.output_variables["output1"].value = numpy.random.uniform(
    input_variables["input1"].value,  # lower dist bound
    input_variables["input2"].value,  # upper dist bound
    (50, 50),
)
self.output_variables["output2"].value = input_variables["input1"].value
self.output_variables["output3"].value = input_variables["input2"].value
```

Because we've introduced numpy as new dependency, add a numpy import to the top of the file so the model.py file looks like:

```python
import copy
from typing import Dict
import numpy as np
from lume_model.models import BaseModel
from lume_model.variables import InputVariable, OutputVariable
from my_model import INPUT_VARIABLES, OUTPUT_VARIABLES

class MyModel(BaseModel):
    input_variables = copy.deepcopy(INPUT_VARIABLES)
    output_variables = copy.deepcopy(OUTPUT_VARIABLES)

    def __init__(self, **settings_kwargs):
        """Initialize the model. If additional settings are required, they can be 
        passed and handled here. For models that are wrapping model loads
        from other frameworks, this can be used for loading weights, referencing
        data files, etc.
        
        """
        super(self).__init__()

        # handle settings if any
        # if settings_kwargs is not None:
        # ...


    def evaluate(
        self, input_variables: Dict[str, InputVariable]
    ) -> Dict[str, OutputVariable]:
        """The evaluate method accepts input variables, performs the model execution,
        then returns a dictionary mapping variable name to output variable.

        Args:
            input_variables (Dict[str, InputVariable]): Dictionary of LUME-model input
                variables with values assigned.

        Returns:
            Dict[str, OutputVariable]: Dictionary of LUME-model output variables with
                values assigned.

        """

        self.output_variables["output1"].value = np.random.uniform(
            input_variables["input1"].value,  # lower dist bound
            input_variables["input2"].value,  # upper dist bound
            (50, 50),
        )
        self.output_variables["output2"].value = input_variables["input1"].value
        self.output_variables["output3"].value = input_variables["input2"].value


        return self.output_variables

```

Add `numpy` to the `dev-environment.yml`, `environment.yml`, and `requirements.txt` files.

## 5. Set up flow

In order for our flow to run, we must edit the code in `my_model/flow.py`. First, delete the `preprocessing_task` code, as we won't be using it. Second, edit `format_file` such that the file holds a string representation of the sum of `output2` and `output3`.

```python

@task(log_stdout=True)
def format_file(output_variables):
    text = str(output_variables["output2"] + output_variables["output3"])
    return text

```

## 3. Set up development environment

Now, create an environment for working with your model package:

```
conda env create -f dev-environment.yml
conda activate my-project-dev
```

Install your package into this environment:
```
pip install -e .
```

## 4. Run your flow

Navigate to the `examples` directory inside the `lume-services-model-template` repository. Open the noteboook `run.ipynb`.



### Notes
 - Jinja templating and github actions

Conda vs pip installation
- runtime requirement should make an effor to only reference conda-forge distributions


